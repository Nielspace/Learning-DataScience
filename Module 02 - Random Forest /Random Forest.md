# The Random Forest Classifier


A big part of machine learning is classification — we want to know what class (a.k.a. group) an observation belongs to. The ability to precisely classify observations is extremely valuable for various business applications like predicting whether a particular user will buy a product or forecasting whether a given loan will default or not.


Data science provides a plethora of classification algorithms such as logistic regression, support vector machine, naive Bayes classifier, and decision trees. But near the top of the classifier hierarchy is the random forest classifier.

Before diving into random forest we need to see what a decision tree is. It’s probably much easier to understand how a decision tree works.

In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. Though a commonly used tool in data mining for deriving a strategy to reach a particular goal, its also widely used in machine learning.

In a decision tree algorithm, the tree takes a binary role before making a decision. For example whether a person is a male or a female. Or whether the fruit is a apple or a banana. Depending upon the task decision tree designs a tree like structure to get to the bottom of all target. 

A decision tree is drawn upside down with its root at the top. 

But decision tree has limitations, like inadequacy in applying regression and predicting continuous values. So to overcome such limitation we combine lots of decision tree together and thus we have a random forest. 

A random forest operates by creating multitude of decision tree to avoid overfitting and give accurate results. 




