{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREAMBLE\n",
    "# Maximum likelihood estimation, naive Bayes\n",
    "# mle.svg\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimator\n",
    "\n",
    "Whereas the last set of notes was about the basic rules of probability, this set of notes will be about some basic techniques in statistics.  For the purposes of this course, statistics essentially means \"probability plus data\".  That The lecture and notes on probability focused on the basic task of modeling random variables and their distributions.  These notes ask a different question: if we are given some collection _data_, how can we find a distribution (or more specifically, find the parameters of a particular class of distribution), that fit this data \"well\".\n",
    "\n",
    "A bit more specifically, our setting here is that we have access to some data set, written as\n",
    "\\begin{equation}\n",
    "x^{(1)}, \\ldots, x^{(m)}.\n",
    "\\end{equation}\n",
    "Our main question is: can I come up with some random variable and distribution $p(X)$ such that the data is well-modeled by this distribution (informally speaking, does the data look like it might be a random sample from this distribution).  In general, this is a hard problem.  If we consider the class of all possible possible possible distributions, there is no real way to efficiently characterize the \"best\" fit to the data.\n",
    "\n",
    "Instead, the common strategy is to pick our distribution from a certain _parameterized family_ of distribution $p(X;\\theta)$, parameterized by $\\theta$, and have our goal be to find the parameters $\\theta$ that fir the data best.  Even here there are multiple different approaches that are possible, but at the very least this gives us a more concrete problem that lets us better attack the underlying problem.\n",
    "\n",
    "In this set of notes, we'll first answer this estimation problem by appeal to the maximum likelihood estimation (MLE) procedure. Then we'll discuss a few additional topics, including the naive Bayes algorithm and how it relates to MLE.  We'll conclude by discussing the strong connections between loss functions in machine learning and MLE methods in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "Given some parameterized distribution $p(X;\\theta)$, and an collection of (independent) samples $x^{(1)},\\ldots,x^{(m)}$, we can compute the probability of observing this set of samples under the distribution, which is simply given by\n",
    "\\begin{equation}\n",
    "p(x^{(1)},\\ldots,x^{(m)};\\theta) = \\prod_{i=1}^m p(x^{(i)};\\theta)\n",
    "\\end{equation}\n",
    "where here we just use the fact that the samples are all assumed to be independent, and we can thus write their joint probability as the product of their probabilities (remember from our last set of notes that this was simply our definition of independence).\n",
    "\n",
    "The basic idea of maximum likelihood estimation, is that _we want to pick parameters $\\theta$ that maximize the probaiblity of the observed data_; in other words, we want to choose $\\theta$ to solve the optimization problem\n",
    "\\begin{equation}\n",
    "\\DeclareMathOperator*{maximize}{maximize}\n",
    "\\maximize_\\theta \\; \\prod_{i=1}^m p(x^{(i)};\\theta)\n",
    "\\end{equation}\n",
    "or equivalently (because maximizing a function is equivalent to maximizing the log of that function, and we can scale this function arbitrarily).\n",
    "\\begin{equation}\n",
    "\\maximize_\\theta \\; \\frac{1}{m} \\sum_{i=1}^m \\log p(x^{(i)};\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "The term we are maximizing above is so common that it usually has it's own name: the _log-likelihood of the data_, written as\n",
    "\\begin{equation}\n",
    "\\ell(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\log p(x^{(i)};\\theta)\n",
    "\\end{equation}\n",
    "where we explicitly write $\\ell$ as a function of $\\theta$ because we want to emphasize the fact this likelihood depends on the parameters.\n",
    "\n",
    "This procedure may seem \"obvious\" when stated like this (of course we want to find the parameters that make the data as likely as possible), but there are actually a number of other estimators that are equally valid or reasonable in many situations.  We'll consider some of these when we discuss hypothesis testing and then later probabilistic modeling, but for now, maximum likelihood estimation will serve as a nice principle for how we fit parameters of distributions to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bernoulli distribution\n",
    "\n",
    "Let's take a simple example as an illustration of this point for the Bernoulli distribution.  Recall that a Bernoulli distribution, $p(X;\\phi)$ is a simple binary distribution over random variables taking values in $\\{0,1\\}$, parameterized by $\\phi$, which is just the probability of the random variable being equal to one. Now suppose we have some data $x^{(1)}, \\ldots, x^{(m)}$ with $x^{(i)} \\in \\{0,1\\})$; what would be a good estimate of the Bernoulli parameter $\\phi$?  For example, maybe we flipped a coin 100 times and 30 of these times it came up heads; what would be a good estimate for the probability that this coin comes up heads?\n",
    "\n",
    "The \"obvious\" answer here is that we just estimate $\\phi$ to be the proportion of 1's in the data\n",
    "\\begin{equation}\n",
    "\\phi = \\frac{\\mbox{# 1's}}{\\mbox{# Total}} = \\frac{\\sum_{i=1}^m x^{(i)}}{m}.\n",
    "\\end{equation}\n",
    "But why is this the case?  If we flip the coin just once, for example, would we expect that we should estimate $\\phi$ to be either zero or one?  Maybe some other estimators exist that can better handle our expectation that the coin \"should\" be unbiased, i.e., have $\\phi = 1/2$.\n",
    "\n",
    "While this is certainly true, in fact that maximum likelihood estimate of $\\phi$ _is_ just the equation above, the number of ones divided by the total number.  So this gives some rationale that at least under the principles of maximum likelihood esimation, we should believe that this is a good estimate.  However, showing that this is in fact the maximum likelihood estimator is a little more involved that you might expect.  Let's go through the derivation to see how this work.\n",
    "\n",
    "First, recall that our objective is to choose $\\phi$ maximize the likelihood, or equivalently the log likelihood of the data, of the observed data $x^{(1)}, \\ldots, x^{(m)}$.  This can be written as the optimization problem\n",
    "\\begin{equation}\n",
    "\\maximize_{\\phi} \\sum_{i=1}^m \\log p(x^{(i)};\\phi).\n",
    "\\end{equation}\n",
    "Recall that the probability under a Bernoulli distribution is just $p(X=1;\\phi) = \\phi$, and $p(X=0;\\phi) 1 - \\phi$, which we can write compactly as\n",
    "\\begin{equation}\n",
    "p(X = x; \\phi) = \\phi^x (1-\\phi)^{(1-x)}\n",
    "\\end{equation}\n",
    "(it's easy to see that this equals $\\phi$ or $x=1$ and $1-\\phi$ for $x=0$).  Plugging this in to our maximum likelihood optimization problem we have\n",
    "\\begin{equation}\n",
    "\\maximize_{\\phi} \\sum_{i=1}^m \\left (x^{(i)}\\log\\phi + (1-x^{(i)}) \\log (1-\\phi) \\right )\n",
    "\\end{equation}\n",
    "\n",
    "In order to maximize this equation, let's take the derivative and set it equal to 0 (though we won't show it, it turns out this function just a single maximum point, which thus must have derivative zero, and so we can find it in this manner).  Via some basic calculus we have\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{d}{d \\phi} \\sum_{i=1}^m \\left (x^{(i)}\\log\\phi + (1-x^{(i)}) \\log (1-\\phi) \\right ) \n",
    "& = \\sum_{i=1}^m \\frac{d}{d \\phi} \\left ( x^{(i)}\\log\\phi + (1-x^{(i)}) \\log (1-\\phi) \\right )  \\\\\n",
    "& = \\sum_{i=1}^m \\left ( \\frac{x^{(i)}}{\\phi} - \\frac{1-x^{(i)}}{1-\\phi} \\right )\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Setting this term equal to zero we have\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\sum_{i=1}^m \\left ( \\frac{x^{(i)}}{\\phi} - \\frac{1-x^{(i)}}{1-\\phi} \\right ) = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & \\frac{\\sum_{i=1}^m x^{(i)}}{\\phi} - \\frac{\\sum_{i=1}^m (1-x^{(i)})} {1-\\phi} = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & (1-\\phi) \\sum_{i=1}^m x^{(i)}  - \\phi \\sum_{i=1}^m (1-x^{(i)}) = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & \\sum_{i=1}^m x^{(i)} = \\phi \\sum_{i=1}^m (x^{(i)} + (1-x^{(i)})) \\\\\n",
    "\\Longrightarrow \\;\\; & \\sum_{i=1}^m x^{(i)} = \\phi m \\\\\n",
    "\\Longrightarrow \\;\\; & \\phi = \\frac{\\sum_{i=1}^m x^{(i)}}{m}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "And there we have it, the surprisingly long proof of the fact that if we want to pick $\\phi$ to maximize the likelihood of the observed data, we need to choose it to be equal to the empirical proportion of the ones.  Of course, the objections we had at the beginning of this section were also valid: and in fact this perhaps is _not_ the best estimate of $\\phi$ if we have very little data, or some prior information about what values $\\phi$ should take.  But it _is_ the estimate of $\\phi$ that maximizes the probability of the observed data, and if this is a bad estimate then it reflects more on the underlying problem with this procedure than with the proof above.  Nonetheless, in the presence of a lot of data, there is actually good reason to use the maximum likelihood estimator, and it is extremely common to use in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example: normal distribution\n",
    "\n",
    "Let's take another example, a one dimensional normal distribution with mean $\\mu$ and variance $\\sigma^2$ (note that we're going to directly estimate $\\sigma^2$, not $\\sigma$ here).  Going through the procedure a bit more quickly this time, noting that the Gaussian density is given by\n",
    "\\begin{equation}\n",
    "p(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2} \\right )\n",
    "\\end{equation}\n",
    "then given some data $x^{(1)}, \\ldots, x^{(m)}$ with $x^{(i)} \\in \\mathbb{R}^n$, the maximum likelihood estimation problem takes the form\n",
    "\\begin{equation}\n",
    "\\maximize_{\\mu, \\sigma^2} \\sum_{i=1}^m \\log p(x^{(i)}; \\mu, \\sigma^2) \\;\\; \\equiv \\;\\; \\maximize_{\\mu, \\sigma^2} \\sum_{i=1}^m \\frac{1}{2} \\left(-\\log(2 \\pi) - \\log \\sigma^2 -\\frac{(x-\\mu)^2}{\\sigma^2} \\right ).\n",
    "\\end{equation}\n",
    "where the optimization objective can also be abbreviated as the log likelihood $\\ell(\\mu,\\sigma^2)$.\n",
    "\n",
    "Let's first take the derivative with respect to $\\mu$ and set it equation to zero\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\ell(\\mu,\\sigma^2)}{\\partial \\mu} = \\sum_{i=1}^m \\frac{x^{(i)} - \\mu}{\\sigma^2}\n",
    "\\end{equation}\n",
    "and setting the equation to zero gives\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m \\frac{x^{(i)} - \\mu}{\\sigma^2} = 0 \\; \\Longrightarrow \\; \\sum_{i=1}^m x^{(i)} = m \\mu \\; \\Longrightarrow \\mu = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}\n",
    "\\end{equation}\n",
    "So we have the fact that the maximum likelihood estimate of the mean $\\mu$ is just equal to the empirical mean of the data.  This may not seem all that surprising, but if you didn't know that the parameter $\\mu$ was in fact the mean of the distribution, this would probably be much less apparent initially.  But this derivation illustrates that in order to maximize the probability of the data, we pick the mean to be equal to the empirical mean.\n",
    "\n",
    "The derivation for $\\sigma$ is similar.  Specifically, the derivative there is given by\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\ell(\\mu,\\sigma^2)}{\\partial \\sigma^2} = \\sum_{i=1}^m \\frac{1}{2} \\left ( -\\frac{1}{\\sigma^2} + \\frac{(x^{(i)} - \\mu)^2}{(\\sigma^2)^2} \\right )\n",
    "\\end{equation}\n",
    "\n",
    "and setting equal to zero gives\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\sum_{i=1}^m \\frac{1}{2} \\left ( -\\frac{1}{\\sigma^2} + \\frac{(x^{(i)} - \\mu)^2}{(\\sigma^2)^2} \\right ) = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & -\\sum_{i=1}^m \\sigma^2 + \\sum_{i=1}^m (x^{(i)} - \\mu)^2 = 0 \\\\\n",
    "\\Longrightarrow \\;\\; & \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu)^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "i.e., the best estimate for the variance parameter of a Gaussian is simply the empirical estimate of the variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "One simple algorithm that results from the integration of maximum likelihood estimation techniques is the naive Bayes algorithm for classification.  It is actually also possible to present naive Bayes in terms of the standard \"three elements\" of a machine learning algorithm, but we won't adopt that approach right here, and will instead focus on its instantiation as a statistical modeling algorithm that results from a few probabilistic assumptions.\n",
    "\n",
    "### Modeling the joint distribution $p(X,Y)$\n",
    "The basic idea of naive Bayes is that we are going to jointly model both the input $x$ and output $y$ of a machine learning prediction problem.  Specifically we assume that the input $X = (X_1,X_2,X_3,\\ldots,X_n)$ are binary, categorical, or Gaussian random variables, and that the output $Y$ is a binary or categorical variable (i.e., we are in the setting of binary or multiclass classification).  And our goal is to model the joint distribution $p(X,Y)$.\n",
    "\n",
    "We are going to do this by first representing the joint distribution as \n",
    "\\begin{equation}\n",
    "p(X,Y) = p(X \\mid Y)p(Y).\n",
    "\\end{equation}\n",
    "Of course, in general $X$ is a high dimensional random variable (consider, for instance, the case that $X$ just consists of $n$ binary variables; to represent this entire distribution we would need to assign a probability to each possible assignment of $X$, which would require $2^n -1$ parameters.  The naive Bayes approach, however, is to make the additional assumption that the individual features $X_i$ are _conditionally independent given $Y$_.  This lets us represent the condition distribution $p(X \\mid Y)$ as\n",
    "\\begin{equation}\n",
    "p(X \\mid Y) = \\prod_{i=1}^n p(X_i \\mid Y).\n",
    "\\end{equation}\n",
    "Because $Y$ is categorical, we can represent $p(X_i \\mid Y=y)$ as a separate distribution for each value of $y$.  Thus, we only need to represent $n*k$ different distributions to completely represent our joint distribution, where $k$ is the number of values that $Y$ takes on..\n",
    "\n",
    "Let's look at this in a bit more detail for the simple case that each $X_i$ and $Y$ are all binary.  In this case, we can represent the entire joint distribution by modeling the different Bernoulli distributions\n",
    "\\begin{equation}\n",
    "p(Y ; \\phi_0), \\;\\; p(X_i  \\mid  Y=0; \\phi_i^0) , \\;\\; p(X_i  \\mid  Y=1; \\phi_i^1).\n",
    "\\end{equation}\n",
    "Furthermore, given a collection of examples $(x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)})$, we can form the maximum likelihood estimates of these parameters simply by \n",
    "\\begin{equation}\n",
    "\\phi_0 = \\frac{\\sum_{j=1}^m y^{(j)}}{m}, \\;\\; \\phi_i^y = \\frac{\\sum_{j=1}^m x_i^{(j)} \\mathrm{1}\\{y^{(j)} = y\\}}{\\sum_{j=1}^m \\mathrm{1}\\{y^{(j)} = y\\}}\n",
    "\\end{equation}\n",
    "where $\\mathrm{1}\\{\\cdot\\}$ denotes the 0-1 indicator, the function that is one if the inner argument is true and zero otherwise.  Although this notation is a bit involved, the basic idea is really quite straightforward: in order to estimate the probability $p(X_i=1 \\mid Y=1)$ (this is precisely the parameter $\\phi_i^1$), we just sum over all the times that $X_i=1$ and $Y=1$, and divide by the number of times that $Y=1$.  Although we don't prove it, you should be able to easily convince yourself that this result follows exactly from the same principle of maximum likelihood that we saw above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "Now that we have modeled the joint probability in the manner described above, how do we use this to make a prediction given some new input $x$.  The difficulty here is that we have only modeled the conditional distribution $p(X \\mid Y)$, not $p(Y \\mid X)$; fortunately, this is precisely that Bayes' rule allows us to compute\n",
    "\\begin{equation}\n",
    "p(Y \\mid X) = \\frac{p(X \\mid Y)p(Y)}{\\sum_y p(X \\mid y) p(y)}.\n",
    "\\end{equation}\n",
    "Making this a bit more concrete in terms of the parameters above, for some specific input x $x$\n",
    "\\begin{equation}\n",
    "p(Y = y \\mid X = x) =  \\frac{(\\phi^0)^y(1-\\phi_0)^{(1-y)} \\prod_{i=1}^n (\\phi_i^y)^{x_i} (1-\\phi_i^y)^{(1-x_i)} }{\n",
    "\\sum_{y'}(\\phi^0)^{y'}(1-\\phi_0)^{(1-y')} \\prod_{i=1}^n (\\phi_i^{y'})^{x_i} (1-\\phi_i^{y'})^{(1-x_i)}}.\n",
    "\\end{equation}\n",
    "If we are just trying to find the most likely class prediction, we don't necessarily even need to compute the precise probabilities here, we can just compute the numerator terms for each class and then predict the class with the maximum probability (though note that the denominator is just the sum of of the numerator terms over all classes, so it's not any harder to compute the actual probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common issues and solutions\n",
    "\n",
    "When implementing naive Bayes there are a few common issues that come up which are important to know how to address.  We don't directly show these in code (as implementing naive Bayes is one of the homework problems for this course), the basic ideas are straightforward.\n",
    "\n",
    "**Log probabilities** The challenge here is that the when we are computing probabilities, the product\n",
    "\\begin{equation}\n",
    "p(y)\\prod_{i=1}^n p(x_i \\mid y)\n",
    "\\end{equation}\n",
    "can be very small, and will quickly go to zero to the numerical precision of floating point numbers.  To resolve this issue, a simple solution is to instead compute the probability in their log space\n",
    "\\begin{equation}\n",
    "\\log p(y) + \\sum_{i=1}^n \\log p(x_i \\mid y).\n",
    "\\end{equation}\n",
    "If you only want to compute the most likely class, you can just compute these log terms for all possible classes and them predict the most likely one.\n",
    "\n",
    "**Laplace smoothing**  The second problem is that, if we have never seen some $X_i = 0$ or $X_i = 1$ (for a particular assignment to $Y$, the corresponding probabilities will be zero (and if you take logs, the result would be negative infinity).  To overcome this, we can employ Laplace smoothing, as we saw in the previous lecture on n-gram modeling.  For example, we could replace the probability estimates with\n",
    "\\begin{equation}\n",
    "\\phi_i^y = \\frac{\\sum_{j=1}^m x_i^{(j)} \\mathrm{1}\\{y^{(j)} = y\\} + 1}{\\sum_{j=1}^m \\mathrm{1}\\{y^{(j)} = y\\} + 2}\n",
    "\\end{equation}\n",
    "which is equivalent to \"hallucinating\" one 0 and one 1 for each of the variables $X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other distributions\n",
    "\n",
    "Although naive Bayes is often presented as \"just counting\", the previous discussion on maximum likelihood estimation connection hopefully makes it clear that we can actually apply any model for the distribution $p(X_i \\mid Y)$, and just estimate its parameters via maximum likelihood estimation.  For example (as you will also do on the homework), if $x_i$ is real-valued $p(X_i \\mid Y)$ as a Gaussian\n",
    "\\begin{equation}\n",
    "p(X_i = x_i \\mid Y =y) = \\mathcal{N}(x_i;\\mu_y \\sigma_y^2)\n",
    "\\end{equation}\n",
    "where $\\mu_y$ and $\\sigma_y^2$ are the parameters of the distribution.  Again, following the maximum likelihood formulation, we can estimate these parameters as\n",
    "\\begin{equation}\n",
    "\\mu_y = \\frac{\\sum_{j=1}^m x_i^{(j)} \\mathrm{1}\\{y^{(j)} = y\\}}{\\sum_{j=1}^m \\mathrm{1}\\{y^{(j)} = y\\}}, \\;\\; \n",
    "\\sigma^2_y = \\frac{\\sum_{j=1}^m (x_i^{(j)}-\\mu_y)^2 \\mathrm{1}\\{y^{(j)} = y\\}}{\\sum_{j=1}^m \\mathrm{1}\\{y^{(j)} = y\\}}\n",
    "\\end{equation}\n",
    "which again are just the standard Gaussian MLE estimates except restricted to just those cases where $y^{(j)} = y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning and maximum likelihood estimation\n",
    "\n",
    "In fact, although we will only discuss it briefly here, there is a strong connection between some of the machine learning methods we have already seen in this course and the principle of maximum likelihood estimation.  We'll just review the connection here briefly for two specific algorithms: logistic regression and least squares.\n",
    "\n",
    "### Logistic regression as maximum likelihood estimation\n",
    "\n",
    "Remember that we previously formulated logistic regression as solving the optimization problem\n",
    "\\begin{equation}\n",
    "\\DeclareMathOperator*{minimize}{minimize}\n",
    "\\minimize_\\theta \\;\\; \\sum_{i=1}^m \\ell_{\\mathrm{logistic}}(h_\\theta(x^{(i)}),y^{(i)}) \\; \\equiv \\; \\minimize_\\theta \\;\\; \\sum_{i=1}^m \\log\\left(1+\\exp(-h_\\theta(x^{(i)})\\cdot y^{(i)})\\right)\n",
    "\\end{equation}\n",
    "where $x^{(i)}\\in \\mathbb{R}^n$ denoted our input data, and $y^{(i)} \\in \\{-1,+1\\}$ are our class labels. We previously motivated the logistic loss largely appealing to the shape of the loss function, shown again below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x117f6da50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"266.017344pt\" version=\"1.1\" viewBox=\"0 0 376.240625 266.017344\" width=\"376.240625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 266.017344 \n",
       "L 376.240625 266.017344 \n",
       "L 376.240625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 34.240625 228.439219 \n",
       "L 369.040625 228.439219 \n",
       "L 369.040625 10.999219 \n",
       "L 34.240625 10.999219 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m8a040cc72b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"67.720625\" xlink:href=\"#m8a040cc72b\" y=\"228.439219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.59375 35.5 \n",
       "L 73.1875 35.5 \n",
       "L 73.1875 27.203125 \n",
       "L 10.59375 27.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-8722\"/>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(60.349531 243.037656)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.680625\" xlink:href=\"#m8a040cc72b\" y=\"228.439219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −2 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(127.309531 243.037656)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.640625\" xlink:href=\"#m8a040cc72b\" y=\"228.439219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(198.459375 243.037656)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.600625\" xlink:href=\"#m8a040cc72b\" y=\"228.439219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(265.419375 243.037656)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"335.560625\" xlink:href=\"#m8a040cc72b\" y=\"228.439219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(332.379375 243.037656)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- $h_θ(x) \\cdot y$ -->\n",
       "     <defs>\n",
       "      <path d=\"M 55.71875 33.015625 \n",
       "L 49.3125 0 \n",
       "L 40.28125 0 \n",
       "L 46.6875 32.671875 \n",
       "Q 47.125 34.96875 47.359375 36.71875 \n",
       "Q 47.609375 38.484375 47.609375 39.5 \n",
       "Q 47.609375 43.609375 45.015625 45.890625 \n",
       "Q 42.4375 48.1875 37.796875 48.1875 \n",
       "Q 30.5625 48.1875 25.265625 43.296875 \n",
       "Q 19.96875 38.421875 18.40625 30.328125 \n",
       "L 12.5 0 \n",
       "L 3.515625 0 \n",
       "L 18.3125 75.984375 \n",
       "L 27.296875 75.984375 \n",
       "L 21.484375 46.09375 \n",
       "Q 24.90625 50.6875 30.21875 53.34375 \n",
       "Q 35.546875 56 41.40625 56 \n",
       "Q 48.640625 56 52.609375 52.09375 \n",
       "Q 56.59375 48.1875 56.59375 41.109375 \n",
       "Q 56.59375 39.359375 56.375 37.359375 \n",
       "Q 56.15625 35.359375 55.71875 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-Oblique-104\"/>\n",
       "      <path d=\"M 45.515625 34.671875 \n",
       "L 14.453125 34.671875 \n",
       "Q 12.359375 20.0625 14.5 13.875 \n",
       "Q 17.1875 6.25 24.46875 6.25 \n",
       "Q 31.78125 6.25 37.359375 13.921875 \n",
       "Q 42.234375 20.65625 45.515625 34.671875 \n",
       "z\n",
       "M 47.015625 42.96875 \n",
       "Q 48.34375 56.84375 46.625 61.71875 \n",
       "Q 43.953125 69.4375 36.765625 69.4375 \n",
       "Q 29.296875 69.4375 23.828125 61.8125 \n",
       "Q 19.53125 55.671875 16.15625 42.96875 \n",
       "z\n",
       "M 38.1875 76.765625 \n",
       "Q 49.90625 76.765625 54.59375 66.40625 \n",
       "Q 59.28125 56.109375 55.71875 37.84375 \n",
       "Q 52.203125 19.625 43.453125 9.28125 \n",
       "Q 34.765625 -1.125 23.046875 -1.125 \n",
       "Q 11.28125 -1.125 6.640625 9.28125 \n",
       "Q 2 19.625 5.515625 37.84375 \n",
       "Q 9.078125 56.109375 17.71875 66.40625 \n",
       "Q 26.421875 76.765625 38.1875 76.765625 \n",
       "z\n",
       "\" id=\"DejaVuSans-Oblique-952\"/>\n",
       "      <path d=\"M 31 75.875 \n",
       "Q 24.46875 64.65625 21.28125 53.65625 \n",
       "Q 18.109375 42.671875 18.109375 31.390625 \n",
       "Q 18.109375 20.125 21.3125 9.0625 \n",
       "Q 24.515625 -2 31 -13.1875 \n",
       "L 23.1875 -13.1875 \n",
       "Q 15.875 -1.703125 12.234375 9.375 \n",
       "Q 8.59375 20.453125 8.59375 31.390625 \n",
       "Q 8.59375 42.28125 12.203125 53.3125 \n",
       "Q 15.828125 64.359375 23.1875 75.875 \n",
       "z\n",
       "\" id=\"DejaVuSans-40\"/>\n",
       "      <path d=\"M 60.015625 54.6875 \n",
       "L 34.90625 27.875 \n",
       "L 50.296875 0 \n",
       "L 39.984375 0 \n",
       "L 28.421875 21.6875 \n",
       "L 8.296875 0 \n",
       "L -2.59375 0 \n",
       "L 24.3125 28.8125 \n",
       "L 10.015625 54.6875 \n",
       "L 20.3125 54.6875 \n",
       "L 30.8125 34.90625 \n",
       "L 49.125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-Oblique-120\"/>\n",
       "      <path d=\"M 8.015625 75.875 \n",
       "L 15.828125 75.875 \n",
       "Q 23.140625 64.359375 26.78125 53.3125 \n",
       "Q 30.421875 42.28125 30.421875 31.390625 \n",
       "Q 30.421875 20.453125 26.78125 9.375 \n",
       "Q 23.140625 -1.703125 15.828125 -13.1875 \n",
       "L 8.015625 -13.1875 \n",
       "Q 14.5 -2 17.703125 9.0625 \n",
       "Q 20.90625 20.125 20.90625 31.390625 \n",
       "Q 20.90625 42.671875 17.703125 53.65625 \n",
       "Q 14.5 64.65625 8.015625 75.875 \n",
       "z\n",
       "\" id=\"DejaVuSans-41\"/>\n",
       "      <path d=\"M 10.6875 40.921875 \n",
       "L 21 40.921875 \n",
       "L 21 28.515625 \n",
       "L 10.6875 28.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-8901\"/>\n",
       "      <path d=\"M 24.8125 -5.078125 \n",
       "Q 18.5625 -15.578125 14.625 -18.1875 \n",
       "Q 10.6875 -20.796875 4.59375 -20.796875 \n",
       "L -2.484375 -20.796875 \n",
       "L -0.984375 -13.28125 \n",
       "L 4.203125 -13.28125 \n",
       "Q 7.953125 -13.28125 10.59375 -11.234375 \n",
       "Q 13.234375 -9.1875 16.5 -3.21875 \n",
       "L 19.28125 2 \n",
       "L 7.171875 54.6875 \n",
       "L 16.703125 54.6875 \n",
       "L 25.78125 12.796875 \n",
       "L 50.875 54.6875 \n",
       "L 60.296875 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-Oblique-121\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(182.790625 256.717344)scale(0.1 -0.1)\">\n",
       "      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-104\"/>\n",
       "      <use transform=\"translate(63.378906 -16.390625)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-952\"/>\n",
       "      <use transform=\"translate(108.94043 0.015625)\" xlink:href=\"#DejaVuSans-40\"/>\n",
       "      <use transform=\"translate(147.954102 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n",
       "      <use transform=\"translate(207.133789 0.015625)\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "      <use transform=\"translate(265.629883 0.015625)\" xlink:href=\"#DejaVuSans-8901\"/>\n",
       "      <use transform=\"translate(316.899414 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"me1fc0c8d8a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"228.439219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"192.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 195.998437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"155.959219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(20.878125 159.758437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"119.719219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 3 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 123.518437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"83.479219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(20.878125 87.278437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"47.239219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 51.038437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me1fc0c8d8a\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 6 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- Loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 19.671875 72.90625 \n",
       "L 19.671875 8.296875 \n",
       "L 55.171875 8.296875 \n",
       "L 55.171875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-76\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798437 130.773125)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use x=\"55.697266\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"116.878906\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"168.978516\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path clip-path=\"url(#p086dc9e7f0)\" d=\"M 34.240625 46.995855 \n",
       "L 67.083868 82.144498 \n",
       "L 88.532517 104.79151 \n",
       "L 104.954139 121.818022 \n",
       "L 118.024409 135.064058 \n",
       "L 129.083868 145.972402 \n",
       "L 138.802787 155.259693 \n",
       "L 147.516301 163.288929 \n",
       "L 155.559544 170.400445 \n",
       "L 162.932517 176.624727 \n",
       "L 169.63522 182.006541 \n",
       "L 176.002787 186.849984 \n",
       "L 182.370355 191.409674 \n",
       "L 188.402787 195.451357 \n",
       "L 194.100084 199.009177 \n",
       "L 199.797382 202.308238 \n",
       "L 205.494679 205.345455 \n",
       "L 211.191976 208.121492 \n",
       "L 216.889274 210.640728 \n",
       "L 222.586571 212.911009 \n",
       "L 228.283868 214.943226 \n",
       "L 234.316301 216.850403 \n",
       "L 240.348733 218.523826 \n",
       "L 246.716301 220.05829 \n",
       "L 253.419003 221.441928 \n",
       "L 260.456841 222.668843 \n",
       "L 268.164949 223.782809 \n",
       "L 276.543328 224.763376 \n",
       "L 285.927111 225.628024 \n",
       "L 296.316301 226.356927 \n",
       "L 308.381166 226.974449 \n",
       "L 322.791976 227.480078 \n",
       "L 340.554139 227.871894 \n",
       "L 364.348733 228.159385 \n",
       "L 369.040625 228.195855 \n",
       "L 369.040625 228.195855 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 34.240625 228.439219 \n",
       "L 34.240625 10.999219 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 369.040625 228.439219 \n",
       "L 369.040625 10.999219 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 34.240625 228.439219 \n",
       "L 369.040625 228.439219 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 34.240625 10.999219 \n",
       "L 369.040625 10.999219 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 284.590625 33.677344 \n",
       "L 362.040625 33.677344 \n",
       "Q 364.040625 33.677344 364.040625 31.677344 \n",
       "L 364.040625 17.999219 \n",
       "Q 364.040625 15.999219 362.040625 15.999219 \n",
       "L 284.590625 15.999219 \n",
       "Q 282.590625 15.999219 282.590625 17.999219 \n",
       "L 282.590625 31.677344 \n",
       "Q 282.590625 33.677344 284.590625 33.677344 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\">\n",
       "     <path d=\"M 286.590625 24.097656 \n",
       "L 306.590625 24.097656 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\"/>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- Zero-one -->\n",
       "     <defs>\n",
       "      <path d=\"M 5.609375 72.90625 \n",
       "L 62.890625 72.90625 \n",
       "L 62.890625 65.375 \n",
       "L 16.796875 8.296875 \n",
       "L 64.015625 8.296875 \n",
       "L 64.015625 0 \n",
       "L 4.5 0 \n",
       "L 4.5 7.515625 \n",
       "L 50.59375 64.59375 \n",
       "L 5.609375 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-90\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 4.890625 31.390625 \n",
       "L 31.203125 31.390625 \n",
       "L 31.203125 23.390625 \n",
       "L 4.890625 23.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-45\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(314.590625 27.597656)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-90\"/>\n",
       "      <use x=\"68.505859\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"130.029297\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"171.111328\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"232.308594\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use x=\"268.408203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"329.589844\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"392.96875\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p086dc9e7f0\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"34.240625\" y=\"10.999219\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hy = np.linspace(-5,5,1000)\n",
    "plt.plot(hy, np.log(1+np.exp(-hy)))\n",
    "plt.xlim([-5,5])\n",
    "plt.ylim([0, 6])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"$h_θ(x) \\cdot y$\")\n",
    "plt.legend(['Zero-one', 'Logistic', 'Hinge', 'Exponential'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is small when $h_\\theta(x)\\cdot y$ is large (meaning we have the correct class prediction with high confidence), and increases linearly as $h_\\theta(x)\\cdot y$ becomes more negative (meaning we have the incorrect prediction, with increasing confidence).  And unlike, for example, the hinge loss, this loss was smooth and differentiable everywhere.\n",
    "\n",
    "But this seems like a somewhat ad-hoc justification, and it turns out there is a way of understanding logistic regression from the perspective of maximum likelihood estimation instead.  The core idea here is we are going to use what is called a _log linear_ model, that is we are going to represent the probability\n",
    "\\begin{equation}\n",
    "p(y=+1 \\mid x;\\theta_+) \\propto \\exp(\\theta^T_+ x)\n",
    "\\end{equation}\n",
    "where $\\theta_+ \\in \\mathbb{R}^n$ denotes a set of parameters encoding the probability of positive class.  The rationale for this log-linear model, at some level is just the fact that, because it's common to take log probabilities as we've seen above, we want these log-probabilities to have a simple form, and a linear model is one such simple form (there are a few other ways of justifying it, but frankly I think this is the most honest justification, just like we justified least-squares loss by the fact that it was mathematically easy to solve for).  One important aspect to notice, though, is that here we are _directly_ modeling the conditional distribution $p(y  \\mid x)$, rather than (as we did in naive Bayes), modeling the joint distribution $p(x,y)$ and then using Bayes' rule to convert this to a conditional distribution.  In case you encounter these terms elsewhere, this puts logistic regression in a class of models called _discriminative_ models, whereas naive Bayes is an instance of a _generative_ model (we will see generative models again when we talk about probabilistic modeling).\n",
    "\n",
    "In order to turn the above into an actual probability, we need to also have a set of parameters for encoding the negative probability\n",
    "\\begin{equation}\n",
    "p(y=-1 \\mid x;\\theta_-) \\propto \\exp(\\theta^T_- x).\n",
    "\\end{equation}\n",
    "with parameterized by the parameter $\\theta_-$.  Then the actual probability of the positive class is given by\n",
    "\\begin{equation}\n",
    "p(y=+1 \\mid x ; \\theta_+, \\theta_i) = \\frac{\\exp(\\theta^T_+ x)}{\\exp(\\theta^T_+ x) + \\exp(\\theta^T_- x)} = \\frac{1}{1 + \\exp((\\theta_- - \\theta_+)^T x)}.\n",
    "\\end{equation}\n",
    "where we get the second equality by dividing both the numerator and denominator by $\\exp(\\theta^T_+ x)$.  Thus, it turns out, we don't actually need both the $\\theta_+$ and $\\theta_-$, but can just represent their _difference_ as the single parameter vector $\\theta = \\theta_+ - \\theta_-$.  Given this, it should be apparent that\n",
    "\\begin{equation}\n",
    "p(y \\mid x;\\theta) = \\frac{1}{1 + \\exp(-\\theta^T x)}.\n",
    "\\end{equation}\n",
    "Let's apply maximum likelihood estimation to estimate the parameters $\\theta$.  This is done by solving the optimization problem\n",
    "\\begin{equation}\n",
    "\\maximize_\\theta \\; \\sum_{i=1}^m \\log p(y^{(i)} \\mid x^{(i)};\\theta) \\; \\equiv \\; \\minimize_\\theta \\;\\sum_{i=1}^m \\log(1+\\exp(-\\theta^T x))\n",
    "\\end{equation}\n",
    "i.e., maximum likelihood estimation under a log-linear model for binary classification correspond exactly to minimizing logistic loss.  This interpretation, however, also gives us a method for computing probabilities of each class under our logistic regression model, which is often viewed as one of the primary advantages of the method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
